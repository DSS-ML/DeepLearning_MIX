{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2f56a4a",
   "metadata": {},
   "source": [
    "\n",
    "## **Transformers**  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **A Brief History of NLP Advancements**  \n",
    "1. **Rule-Based Era** (1950s–1990s): Handcrafted grammar rules (e.g., ELIZA)  \n",
    "2. **Statistical NLP** (1990s–2010s): Hidden Markov Models, TF-IDF, Word2Vec  \n",
    "3. **Neural Networks** (2010s–2017): RNNs, LSTMs, GRUs (sequential processing bottlenecks)  \n",
    "4. **Transformer Revolution** (2017–Present): Parallel processing, self-attention, scalability  \n",
    "\n",
    "---\n",
    "\n",
    "### **Why Transformers Revolutionized NLP**  \n",
    "- **Parallelization**: Process entire sequences at once (vs. sequential RNNs)  \n",
    "- **Long-Range Dependencies**: Direct token-to-token attention across arbitrary distances  \n",
    "- **Transfer Learning**: Pretrain on massive corpora, fine-tune for downstream tasks  \n",
    "- **State-of-the-Art Performance**: Dominance in GLUE, SQuAD, and other benchmarks  \n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### **Core Idea of Transformers**  \n",
    "Transformers (Vaswani et al., 2017) revolutionized sequence modeling by replacing recurrence (RNNs) and convolution (CNNs) with **self-attention**. This allows:  \n",
    "- **Parallel processing** of entire sequences (no sequential dependency).  \n",
    "- **Global context capture**: Directly model relationships between any two tokens in a sequence, regardless of distance.  \n",
    "- **Scalability**: Efficient handling of long-range dependencies (critical for text, time series, or genomic data).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Self-Attention: The Engine of Transformers**  \n",
    "\n",
    "#### **Key Concepts**  \n",
    "- **Queries (Q), Keys (K), Values (V)**: Learnable representations of the input.  \n",
    "  - **Q** asks: \"What am I looking for?\"  \n",
    "  - **K** answers: \"What do I contain?\"  \n",
    "  - **V** provides: \"The actual information to aggregate.\"  \n",
    "\n",
    "- **Scaled Dot-Product Attention**:  \n",
    "  $$\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$  \n",
    "  - **Softmax**: Assigns weights to values based on query-key compatibility.  \n",
    "  - **Scaling by $\\sqrt{d_k}$**: Prevents gradient vanishing/exploding as dimensionality grows.  \n",
    "\n",
    "#### **Intuition**  \n",
    "- Each token generates a **query** to \"ask\" about other tokens.  \n",
    "- The **keys** determine how much each token \"responds\" to the query.  \n",
    "- The **values** are summed based on these weights, creating a context-aware representation.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Multi-Head Attention**  \n",
    "Extends self-attention by running multiple attention operations in parallel:  \n",
    "1. **Split** Q, K, V into $h$ heads (e.g., $h=8$).  \n",
    "2. **Process independently**: Each head learns different relationship types (e.g., syntactic, semantic).  \n",
    "3. **Concatenate** outputs and project back to original dimension.  \n",
    "\n",
    "**Why It Works**:  \n",
    "- Captures diverse interaction patterns (e.g., long vs. short-range, hierarchical).  \n",
    "- Analogous to CNN filters learning edges vs. textures, but for sequence relationships.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Positional Encoding**  \n",
    "**Problem**: Self-attention treats sequences as unordered sets.  \n",
    "**Solution**: Inject positional information using sinusoidal functions:  \n",
    "$$PE_{(pos,2i)} = \\sin(pos/10000^{2i/d})$$  \n",
    "$$PE_{(pos,2i+1)} = \\cos(pos/10000^{2i/d})$$  \n",
    "- **Learned Alternatives**: Trainable embeddings for positions.  \n",
    "- **Key Property**: Relative distances are preserved via trigonometric identities.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Transformer vs. RNNs/CNNs**  \n",
    "\n",
    "| **Aspect**          | **RNNs**               | **CNNs**               | **Transformers**       |  \n",
    "|----------------------|------------------------|------------------------|------------------------|  \n",
    "| **Computation**      | Sequential             | Local windows          | Fully parallel         |  \n",
    "| **Context Range**    | Limited by hidden state | Kernel size            | Global (all tokens)    |  \n",
    "| **Memory**           | O(n)                   | O(kn)                  | O(n²)                  |  \n",
    "| **Use Case**         | Short sequences        | Local patterns         | Long-range dependencies|  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e53e3109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        # Projections for Q, K, V\n",
    "        self.to_qkv = nn.Linear(embed_size, 3 * embed_size)\n",
    "        self.scale = self.head_dim ** -0.5  # 1/sqrt(d_k)\n",
    "        \n",
    "        # Final output projection\n",
    "        self.to_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)  # Split into Q, K, V\n",
    "        q, k, v = map(lambda t: t.view(batch_size, seq_len, self.heads, self.head_dim).transpose(1, 2), qkv)\n",
    "\n",
    "        # Attention scores\n",
    "        scores = (q @ k.transpose(-2, -1)) * self.scale #@ is matrix multiplication, * is element multiplication \n",
    "        if mask is not None:  # Mask future tokens (e.g., for decoding)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        context = attn_weights @ v  # Aggregate values\n",
    "\n",
    "        # Reassemble and project\n",
    "        context = context.transpose(1, 2).reshape(batch_size, seq_len, self.embed_size)\n",
    "        return self.to_out(context)\n",
    "\n",
    "# Example usage\n",
    "embed_size = 512\n",
    "heads = 8\n",
    "x = torch.randn(2, 10, embed_size)  # (batch, sequence length, features)\n",
    "sa = SelfAttention(embed_size, heads)\n",
    "output = sa(x)\n",
    "print(output.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80a28c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word 'The' pays most attention to:\n",
      "- in: 0.21\n",
      "- sleeps: 0.12\n",
      "- carpet: 0.11\n",
      "\n",
      "Word 'black' pays most attention to:\n",
      "- black: 0.11\n",
      "- garden: 0.10\n",
      "- cat: 0.10\n",
      "\n",
      "Word 'cat' pays most attention to:\n",
      "- barks: 0.20\n",
      "- black: 0.12\n",
      "- dog: 0.11\n",
      "\n",
      "Word 'sleeps' pays most attention to:\n",
      "- in: 0.20\n",
      "- sleeps: 0.17\n",
      "- while: 0.10\n",
      "\n",
      "Word 'on' pays most attention to:\n",
      "- cat: 0.20\n",
      "- on: 0.10\n",
      "- while: 0.10\n",
      "\n",
      "Word 'the' pays most attention to:\n",
      "- cat: 0.12\n",
      "- black: 0.11\n",
      "- The: 0.10\n",
      "\n",
      "Word 'carpet' pays most attention to:\n",
      "- sleeps: 0.13\n",
      "- cat: 0.13\n",
      "- in: 0.12\n",
      "\n",
      "Word 'while' pays most attention to:\n",
      "- cat: 0.16\n",
      "- while: 0.10\n",
      "- sleeps: 0.09\n",
      "\n",
      "Word 'dog' pays most attention to:\n",
      "- cat: 0.18\n",
      "- in: 0.15\n",
      "- sleeps: 0.10\n",
      "\n",
      "Word 'barks' pays most attention to:\n",
      "- cat: 0.27\n",
      "- on: 0.09\n",
      "- garden: 0.08\n",
      "\n",
      "Word 'in' pays most attention to:\n",
      "- sleeps: 0.14\n",
      "- The: 0.11\n",
      "- while: 0.10\n",
      "\n",
      "Word 'garden' pays most attention to:\n",
      "- cat: 0.13\n",
      "- garden: 0.09\n",
      "- on: 0.09\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Longer sentence\n",
    "sentence = \"The black cat sleeps on the carpet while the dog barks in the garden\"\n",
    "vocab = {word: idx for idx, word in enumerate(sentence.split())}\n",
    "\n",
    "embed_size = 8\n",
    "heads = 2\n",
    "\n",
    "# Word embeddings\n",
    "word_embeddings = torch.randn(len(vocab), embed_size)\n",
    "x = word_embeddings.unsqueeze(0)\n",
    "\n",
    "sa = SelfAttention(embed_size, heads)\n",
    "with torch.no_grad():\n",
    "    qkv = sa.to_qkv(x).chunk(3, dim=-1)\n",
    "    q, k, v = map(lambda t: t.view(1, len(vocab), heads, embed_size//heads).transpose(1, 2), qkv)\n",
    "    scores = (q @ k.transpose(-2, -1)) * sa.scale\n",
    "    attention_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "# Show strongest relationships\n",
    "for i, word1 in enumerate(vocab):\n",
    "    print(f\"\\nWord '{word1}' pays most attention to:\")\n",
    "    weights = attention_weights[0, 0, i]\n",
    "    # Get top 3 weights\n",
    "    top_indices = weights.topk(3).indices\n",
    "    for idx in top_indices:\n",
    "        word2 = list(vocab.keys())[idx]\n",
    "        weight = weights[idx].item()\n",
    "        print(f\"- {word2}: {weight:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9380841",
   "metadata": {},
   "source": [
    "**p.s.** Words are repeated because the model is also learning self-attention - meaning how much a word should pay attention to itself. \n",
    "\n",
    "Word 'black' pays most attention to:\n",
    "- black: 0.11  # Self-attention \n",
    "\n",
    "This happens because:\n",
    "1. Sometimes a word's context includes the word itself\n",
    "2. The weights are probabilities (sum to 1), distributed across all words\n",
    "3. We're using random embeddings (`torch.randn`), so the patterns aren't semantically meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f6189",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **7. Why Attention Works: The Big Picture**  \n",
    "1. **Dynamic Weighting**: Unlike fixed convolution kernels, attention adaptively focuses on relevant tokens.  \n",
    "2. **Interpretability**: Attention maps reveal which tokens influence predictions (e.g., subject-verb agreement in NLP).  \n",
    "3. **Flexibility**: Handles variable-length inputs and cross-modal data (text + images).  \n",
    "\n",
    "---\n",
    "\n",
    "### **8. Limitations & Extensions**  \n",
    "- **Quadratic Complexity**: Infeasible for very long sequences (e.g., genome data).  \n",
    "  - **Fix**: Sparse attention (Longformer, BigBird).  \n",
    "- **Lack of Inductive Bias**: Requires more data than RNNs/CNNs.  \n",
    "  - **Fix**: Hybrid architectures (Conformer).  \n",
    "\n",
    "---\n",
    "\n",
    "Transformers’ generality makes them applicable to **any data with sequential or relational structure**—from text to financial time series—by rethinking how we model dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62095cb1",
   "metadata": {},
   "source": [
    "## **Key Transformer Architectures**  \n",
    "### **BERT (Bidirectional Encoder)**  \n",
    "- **Pretraining**:  \n",
    "  - **Masked Language Modeling (MLM)**: Predict masked tokens (e.g., `\"The [MASK] sat on the mat\"`)  \n",
    "  - **Next Sentence Prediction (NSP)**: Classify if two sentences are consecutive  \n",
    "- **Bidirectionality**: Context from left and right (critical for tasks like NER)  \n",
    "- **Applications**:  \n",
    "  - Sentiment analysis (economic indicator prediction from news)  \n",
    "  - Document classification (regulatory compliance checks)  \n",
    "\n",
    "---\n",
    "\n",
    "### **GPT Series (Autoregressive Decoders)**  \n",
    "- **Unidirectional**: Predict next token using left-to-right context  \n",
    "- **Generative Strength**: ChatGPT, code generation, creative writing  \n",
    "- **Economics Applications**:  \n",
    "  - Financial report generation  \n",
    "  - Synthetic data creation for policy simulations  \n",
    "\n",
    "---\n",
    "\n",
    "### **BART (Seq2Seq Architecture)**  \n",
    "- **Denoising Pretraining**: Corrupt text (e.g., masking, permutation), then reconstruct  \n",
    "- **Encoder-Decoder**: Combines BERT’s encoder with GPT-style decoder  \n",
    "- **Applications**:  \n",
    "  - Summarizing economic research papers  \n",
    "  - Generating executive briefs from lengthy reports  \n",
    "\n",
    "---\n",
    "\n",
    "### **T5 (Text-to-Text Framework)**  \n",
    "- **Unified Framework**: All tasks formatted as `\"input: text → output: text\"`  \n",
    "  - Example: `\"Translate English to German: Hello → Hallo\"`  \n",
    "- **Applications**:  \n",
    "  - Multilingual economic survey translation  \n",
    "  - Converting unstructured data (emails) to structured tables  \n",
    "\n",
    "---\n",
    "\n",
    "### **Efficient Models**  \n",
    "| **Model**     | **Key Innovation**                     | **Use Case**                |  \n",
    "|---------------|----------------------------------------|-----------------------------|  \n",
    "| **DistilBERT**| Knowledge distillation (60% smaller)  | Real-time sentiment analysis|  \n",
    "| **ALBERT**    | Parameter sharing (89% fewer params)  | Low-resource language tasks |  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095e713d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Fine-Tuning BERT for a Specific Task\n",
    "\n",
    "Fine-tuning involves adapting a large, pre-trained model like BERT (Devlin et al., 2018) to a particular downstream task such as text classification. The code below demonstrates a basic setup using Hugging Face's Transformers library for binary/multiclassification:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4870ecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers accelerate --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a824eec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82fec80ab4874f84b4d8e779e988462b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f602a0c0acad47b18fe0295bc40e1494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd33c60d542e45559871d3c027ce5b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8af5aaa713a4c2e93d7f5a617ae4041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72011d44854845d180d9e0e2dfc1b023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4730417f80f422ea732dd707f80694a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39311558fe794bf599827e848b84fcfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b0b3104b0641d0858ad052810da6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9aaa6d550ce4da7a5c3185172a2bb96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c4c4bfb1ee413eb05cdb18438b7c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "\n",
      "Test prediction for 'This is an amazing movie!':\n",
      "Positive: 88.86%\n",
      "Negative: 11.14%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128, device='cpu'):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten().to(self.device),\n",
    "            'attention_mask': encoding['attention_mask'].flatten().to(self.device),\n",
    "            'label': torch.tensor(label, dtype=torch.long, device=self.device)\n",
    "        }\n",
    "\n",
    "def train_sentiment_model(data, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    # Initialize BERT\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = SentimentDataset(\n",
    "        data['text'].values, \n",
    "        data['label'].values, \n",
    "        tokenizer,\n",
    "        device=device\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    \n",
    "    # Training setup\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    print(f\"Starting training on device: {device}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['label']\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'text': [\n",
    "        'This movie was great!',\n",
    "        'Terrible waste of time',\n",
    "        'I loved this film',\n",
    "        'Worst movie ever',\n",
    "        'Highly recommended',\n",
    "    ],\n",
    "    'label': [1, 0, 1, 0, 1]  # 1 for positive, 0 for negative\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Train model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, tokenizer = train_sentiment_model(df, device)\n",
    "\n",
    "# Test prediction\n",
    "def predict_sentiment(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    return {\n",
    "        'positive': prediction[0][1].item(),\n",
    "        'negative': prediction[0][0].item()\n",
    "    }\n",
    "\n",
    "# Test the model\n",
    "test_text = \"This is an amazing movie!\"\n",
    "prediction = predict_sentiment(test_text, model, tokenizer, device)\n",
    "print(f\"\\nTest prediction for '{test_text}':\")\n",
    "print(f\"Positive: {prediction['positive']:.2%}\")\n",
    "print(f\"Negative: {prediction['negative']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c405c1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|███████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.29it/s, loss=0.9519]\n",
      "Epoch 2/10: 100%|███████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.78it/s, loss=0.9562]\n",
      "Epoch 3/10: 100%|███████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.58it/s, loss=0.8871]\n",
      "Epoch 4/10: 100%|███████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.98it/s, loss=0.8717]\n",
      "Epoch 5/10: 100%|███████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.55it/s, loss=1.0387]\n",
      "Epoch 6/10: 100%|███████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.46it/s, loss=0.5864]\n",
      "Epoch 7/10: 100%|███████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.32it/s, loss=0.6808]\n",
      "Epoch 8/10: 100%|███████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.18it/s, loss=0.3695]\n",
      "Epoch 9/10: 100%|███████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.91it/s, loss=0.5449]\n",
      "Epoch 10/10: 100%|██████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.55it/s, loss=0.4143]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "\n",
      "Test prediction for 'This movie was interesting but had some flaws':\n",
      "Negative: 11.53%\n",
      "Neutral: 47.35%\n",
      "Positive: 41.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128, device='cpu'):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten().to(self.device),\n",
    "            'attention_mask': encoding['attention_mask'].flatten().to(self.device),\n",
    "            'label': torch.tensor(label, dtype=torch.long, device=self.device)\n",
    "        }\n",
    "\n",
    "def train_sentiment_model(data, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    dataset = SentimentDataset(\n",
    "        data['text'].values, \n",
    "        data['label'].values, \n",
    "        tokenizer,\n",
    "        device=device\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    print(f\"Starting training on device: {device}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['label']\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# Sample data with neutral class\n",
    "data = {\n",
    "    'text': [\n",
    "        'This movie was great!',\n",
    "        'Terrible waste of time',\n",
    "        'The film was okay, nothing special',\n",
    "        'I loved this film',\n",
    "        'Worst movie ever',\n",
    "        'It was decent, had its moments',\n",
    "        'Highly recommended',\n",
    "        'Average movie, wouldn\\'t watch again',\n",
    "    ],\n",
    "    'label': [2, 0, 1, 2, 0, 1, 2, 1]  # 0: negative, 1: neutral, 2: positive\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, tokenizer = train_sentiment_model(df, device)\n",
    "\n",
    "def predict_sentiment(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    return {\n",
    "        'negative': prediction[0][0].item(),\n",
    "        'neutral': prediction[0][1].item(),\n",
    "        'positive': prediction[0][2].item()\n",
    "    }\n",
    "\n",
    "# Test prediction\n",
    "test_text = \"This movie was interesting but had some flaws\"\n",
    "prediction = predict_sentiment(test_text, model, tokenizer, device)\n",
    "print(f\"\\nTest prediction for '{test_text}':\")\n",
    "print(f\"Negative: {prediction['negative']:.2%}\")\n",
    "print(f\"Neutral: {prediction['neutral']:.2%}\")\n",
    "print(f\"Positive: {prediction['positive']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef81081",
   "metadata": {},
   "source": [
    "### **Swapping Models (BERT → DistilBERT)**  \n",
    "try by yourself :)\n",
    "```python\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Only change these lines:\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
